{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from scipy.optimize import Bounds, LinearConstraint, minimize\n",
    "import scipy.sparse.linalg as sparla\n",
    "import cvxpy as cp\n",
    "import os\n",
    "import yfinance as yf\n",
    "import itertools, sys\n",
    "import sys\n",
    "import threading\n",
    "import time\n",
    "from itertools import cycle\n",
    "\n",
    "global ANNUALIZATION_FACTOR\n",
    "ANNUALIZATION_FACTOR = 12\n",
    "\n",
    "class Spinner:\n",
    "    def __init__(self, message=\"Processing...\", color=\"white\"):\n",
    "        self.spinner = cycle(['|', '/', '-', '\\\\'])\n",
    "        self.stop_running = threading.Event()\n",
    "        self.message_text = message\n",
    "        self.lock = threading.Lock()  # To prevent conflicts with message updates\n",
    "        self.color_code = {\n",
    "            \"red\": \"\\033[31m\",\n",
    "            \"green\": \"\\033[32m\",\n",
    "            \"yellow\": \"\\033[33m\",\n",
    "            \"blue\": \"\\033[34m\",\n",
    "            \"white\": \"\\033[37m\",\n",
    "            \"reset\": \"\\033[0m\"\n",
    "        }\n",
    "        self.current_color = color \n",
    "\n",
    "    def start(self):\n",
    "        def run_spinner():\n",
    "            sys.stdout.write(self.message_text + \" \")\n",
    "            while not self.stop_running.is_set():\n",
    "                with self.lock:\n",
    "                    colored_symbol = self.color_code.get(self.current_color, self.color_code[\"white\"]) + next(self.spinner) + self.color_code[\"reset\"]\n",
    "                    sys.stdout.write(colored_symbol)  \n",
    "                    sys.stdout.flush()\n",
    "                    sys.stdout.write('\\b')\n",
    "                time.sleep(0.1)\n",
    "\n",
    "        self.thread = threading.Thread(target=run_spinner)\n",
    "        self.thread.start()\n",
    "\n",
    "    def stop(self):\n",
    "        self.stop_running.set()\n",
    "        self.thread.join()\n",
    "\n",
    "    def message(self, new_message, color=\"white\"):\n",
    "        \"\"\"Update the status message and color while the spinner is running.\"\"\"\n",
    "        with self.lock:\n",
    "            sys.stdout.write('\\b \\b')  \n",
    "            sys.stdout.flush()\n",
    "            self.current_color = color\n",
    "            colored_message = self.color_code.get(color, self.color_code[\"white\"]) + new_message + self.color_code[\"reset\"]\n",
    "            sys.stdout.write('\\r' + colored_message + \" \")\n",
    "            sys.stdout.flush()\n",
    "            time.sleep(0.1)\n",
    "            self.message_text = new_message\n",
    "\n",
    "    def erase(self):\n",
    "        \"\"\"Erase the current message from the terminal.\"\"\"\n",
    "        with self.lock:\n",
    "            sys.stdout.write('\\r')\n",
    "            sys.stdout.write(' ' * (len(self.message_text) + 2))\n",
    "            sys.stdout.write('\\r')\n",
    "            sys.stdout.flush()\n",
    "            self.message_text = \"\"\n",
    "\n",
    "def excel_loader(path):\n",
    "    data = pd.read_excel(path, usecols=lambda x: x != 'NAME', index_col=0).transpose()\n",
    "    data.index = pd.to_datetime(data.index, format='%Y')\n",
    "    data.index = data.index + pd.offsets.YearEnd()\n",
    "    data.index.rename('DATE', inplace=True)\n",
    "    data = data[data.index.year > 2004]\n",
    "    nan_columns = data.iloc[0].loc[data.iloc[0].isna()].index\n",
    "    data.loc['2005-12-31', nan_columns] = data.loc['2006-12-31', nan_columns]\n",
    "    data.interpolate(method='linear', axis=0, inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "def annualized_mean(sample_mean: float) -> float:\n",
    "    return (1 + sample_mean) ** ANNUALIZATION_FACTOR - 1\n",
    "\n",
    "def annualized_volatility(sample_std: float) -> float:\n",
    "    return sample_std * np.sqrt(ANNUALIZATION_FACTOR)\n",
    "\n",
    "def sharpe_ratio(mean: float, volatility: float) -> float:\n",
    "    return mean / volatility\n",
    "\n",
    "def portfolio_evaluation(monthlyReturns: pd.Series | np.ndarray, monthlyRFrate: pd.Series) -> dict:\n",
    "\n",
    "    '''\n",
    "    Evaluates the performance of a portfolio given its monthly returns. \n",
    "    It calculates and returns a dictionary containing the annualized mean return,\n",
    "    annualized volatility, Sharpe ratio, minimum return, and maximum return of the portfolio.\n",
    "    monthlyRFrate must be indexed by and be of same length as the sample of monthly returns \n",
    "    that is being evaluated.\n",
    "    '''\n",
    "\n",
    "    mean = monthlyReturns.mean()\n",
    "    volatility = monthlyReturns.std()\n",
    "    annualizedMean = annualized_mean(mean)\n",
    "    annualizedVolatility = annualized_volatility(volatility)\n",
    "    monthlyExcessReturn = monthlyReturns.sub(monthlyRFrate, axis=0)\n",
    "    meanExcessReturn = monthlyExcessReturn.mean()\n",
    "    annualizedExcessReturn = annualized_mean(meanExcessReturn)\n",
    "    sharpeRatio = sharpe_ratio(annualizedExcessReturn, annualizedVolatility)\n",
    "    minimum = monthlyReturns.min()\n",
    "    maximum = monthlyReturns.max()\n",
    "\n",
    "    portfolio_performance = {\n",
    "        'mu': annualizedMean,\n",
    "        'std': annualizedVolatility,\n",
    "        'SR': sharpeRatio,\n",
    "        'min': minimum,\n",
    "        'max': maximum\n",
    "    }\n",
    "\n",
    "    return portfolio_performance\n",
    "\n",
    "def create_filter_mask(sampleData, marketValuesData, minMarketCap: float = -np.inf, maxMarketCap: float = np.inf):\n",
    "    # Identify the latest date in both sampleData and marketValuesData\n",
    "    latestDateSample = sampleData.index.max()\n",
    "\n",
    "    # Zero December Returns filter (activated/deactivated based on criteria)\n",
    "    decemberData = sampleData.loc[[latestDateSample]]\n",
    "    decemberFilter = decemberData.columns[decemberData.iloc[0] == np.inf]  # deactivated\n",
    "\n",
    "    # December price below threshold filter\n",
    "    yearEndPrices = sampleData.loc[latestDateSample]\n",
    "    priceFilter = yearEndPrices[yearEndPrices < -np.inf].index  # activated\n",
    "\n",
    "    # High return filter (both high and low extremes)\n",
    "    returnFilterHigh = sampleData.columns[sampleData.max() >= np.inf]  # deactivated\n",
    "    returnFilterLow = sampleData.columns[sampleData.min() <= -np.inf]  # deactivated\n",
    "    returnFilter = returnFilterHigh.union(returnFilterLow)\n",
    "    \n",
    "    # Frequent Zero Returns filter\n",
    "    startOfYear = pd.Timestamp(latestDateSample.year, 1, 1)\n",
    "    yearlyData = sampleData.loc[startOfYear:latestDateSample]\n",
    "    monthsWithZeroReturns = (yearlyData == 0).sum(axis=0)\n",
    "    frequentZerosFilter = monthsWithZeroReturns[monthsWithZeroReturns >= 12].index  # activated\n",
    "\n",
    "    # Market Cap filters based on the latest date in marketValuesData\n",
    "    marketValuesAtEnd = marketValuesData.loc[latestDateSample]\n",
    "    marketCapFilterMin = marketValuesAtEnd[marketValuesAtEnd < minMarketCap].index\n",
    "    marketCapFilterMax = marketValuesAtEnd[marketValuesAtEnd > maxMarketCap].index\n",
    "\n",
    "    # Combine all filters\n",
    "    combinedFilter = decemberFilter.union(frequentZerosFilter).union(priceFilter).union(returnFilter)\n",
    "    combinedFilter = combinedFilter.union(marketCapFilterMin).union(marketCapFilterMax)\n",
    "    \n",
    "    # Return the combined filter\n",
    "    return combinedFilter\n",
    "\n",
    "class Portfolio():\n",
    "    valid_types = ('markowitz', 'erc', 'max_sharpe', 'min_var')\n",
    "    non_combined_portfolios = []\n",
    "    gamma_linspace = np.linspace(-0.5, 1.5, 101)\n",
    "    \n",
    "    def __init__(self, returns: pd.DataFrame | pd.Series, type: str='markowitz', names: list[str]=None, trust_markowitz: bool=False, resample: bool=False, target_gamma=None):\n",
    "        assert type.lower() in self.valid_types, f\"Invalid type: {type}. Valid types are: {self.valid_types}\"\n",
    "        #TODO: Attention! ERC portfolios use sample returns, not ex-ante expectations.\n",
    "        self.trust_markowitz = trust_markowitz\n",
    "        self.gamma = self.assign_gamma(target_gamma)\n",
    "        self.resample = resample\n",
    "        self.type = type.lower()\n",
    "        self.ticker = returns.columns\n",
    "        self.returns = returns\n",
    "        self.expected_returns = self.get_expected_returns()\n",
    "        self.expected_covariance = self.get_expected_covariance()\n",
    "        self.dim = len(self.expected_returns)\n",
    "        self.len = len(self.returns)\n",
    "\n",
    "        self.frontier = ... # Frontier is calulated in get_optimal()\n",
    "        self.optimal_weights = self.get_optimal()\n",
    "        self.expected_portfolio_return = self.get_expected_portfolio_return()\n",
    "        self.expected_portfolio_varcov = self.get_expected_portfolio_varcov()\n",
    "\n",
    "        if self.type != 'erc':\n",
    "            Portfolio.non_combined_portfolios.append(self)\n",
    "        if self.type == 'erc':\n",
    "            self.frontier.loc[0, 'expected_return'] = self.expected_portfolio_return\n",
    "            self.frontier.loc[0, 'expected_variance'] = self.expected_portfolio_varcov\n",
    "            self.frontier.loc[0, 'expected_sharpe'] = self.expected_portfolio_return / np.sqrt(self.expected_portfolio_varcov)\n",
    "            for i, asset in enumerate(self.ticker):\n",
    "                self.frontier.loc[0, asset] = self.optimal_weights[i]\n",
    "\n",
    "    def assign_gamma(self, gamma):\n",
    "        \"\"\"Assigns the closest gamma value from gamma_linspace to the provided target_gamma.\"\"\"\n",
    "        if gamma is None:\n",
    "            return None\n",
    "        return min(self.gamma_linspace, key=lambda x: abs(x - gamma))\n",
    "\n",
    "    def get_optimal(self):\n",
    "        self.frontier = self.get_frontier()\n",
    "        if self.type == 'markowitz':\n",
    "            assert self.gamma is not None, \"Markowitz optimization requires a gamma value.\"\n",
    "            return self.frontier.loc[self.gamma, self.ticker].values\n",
    "        if self.type == 'min_var':\n",
    "            return self.frontier.loc[self.frontier['expected_variance'].idxmin(), self.ticker].values\n",
    "        if self.type == 'max_sharpe':\n",
    "            return self.frontier.loc[self.frontier['expected_sharpe'].idxmax(), self.ticker].values\n",
    "        if self.type == 'erc':\n",
    "            return self._fit_erc()\n",
    "\n",
    "    def get_frontier(self, singular=None):\n",
    "        \"\"\"Calculate the efficient frontier.\"\"\"\n",
    "        method = self._frontier_method()\n",
    "        if self.resample:\n",
    "            frontier_weights = self._resample(method)\n",
    "        else:\n",
    "            frontier_weights = method(singular)\n",
    "        return self._pickle_frontier(frontier_weights, singular)\n",
    "    \n",
    "    def _frontier_method(self):\n",
    "        if self.dim >= 30:\n",
    "            return self._efficient_frontier_cvxpy\n",
    "        else:\n",
    "            return self._efficient_frontier_scipy\n",
    "        \n",
    "    def _pickle_frontier(self, frontier_weights: np.ndarray, singular=None) -> pd.DataFrame:\n",
    "        \"\"\"Helper method to create a DataFrame from the efficient frontier weights.\"\"\"\n",
    "        if singular is not None:\n",
    "            frontier_weights = frontier_weights.reshape(1, -1)\n",
    "\n",
    "        expected_returns_vector = frontier_weights @ self.expected_returns\n",
    "        expected_variances_vector = np.einsum('ij,jk,ik->i', frontier_weights, self.expected_covariance, frontier_weights)\n",
    "        \n",
    "        expected_sharpe = np.zeros_like(expected_returns_vector)\n",
    "        non_zero_variance = expected_variances_vector > 0\n",
    "        expected_sharpe[non_zero_variance] = (expected_returns_vector[non_zero_variance] / np.sqrt(expected_variances_vector[non_zero_variance]))\n",
    "\n",
    "        data = {\n",
    "            'gamma': self.gamma_linspace if not singular else [singular],\n",
    "            'expected_return': expected_returns_vector,\n",
    "            'expected_variance': expected_variances_vector,\n",
    "            'expected_sharpe': expected_sharpe}\n",
    "        \n",
    "        weight_columns = {f'{asset}': frontier_weights[:, i] for i, asset in enumerate(self.ticker)}\n",
    "        data.update(weight_columns)\n",
    "\n",
    "        frontier_df = pd.DataFrame(data)\n",
    "        frontier_df.set_index('gamma', inplace=True)        \n",
    "        return frontier_df\n",
    "        \n",
    "    def _efficient_frontier_scipy(self, singular=None):\n",
    "        initial_guess = np.ones(self.dim) / self.dim \n",
    "        constraints = [LinearConstraint(np.ones(self.dim), 1, 1)]\n",
    "        bounds = Bounds(0, 1)\n",
    "\n",
    "        if not singular:\n",
    "            results = np.zeros((len(self.gamma_linspace), self.dim))\n",
    "            itterator = enumerate(self.gamma_linspace)\n",
    "        else:\n",
    "            itterator = [(0, singular)]\n",
    "\n",
    "        for i, gamma in itterator:\n",
    "            def objective(weights):\n",
    "                return 0.5 * np.dot(weights.T, np.dot(self.expected_covariance, weights)) - gamma * np.dot(self.expected_returns, weights)\n",
    "            def jacobian(weights):\n",
    "                return np.dot(self.expected_covariance, weights) - gamma * self.expected_returns\n",
    "\n",
    "            kwargs = {'fun': objective,\n",
    "                      'jac': jacobian,\n",
    "                      'x0': initial_guess,\n",
    "                      'constraints': constraints,\n",
    "                      'bounds': bounds,\n",
    "                      'method': 'SLSQP',\n",
    "                      'tol': 1e-16}\n",
    "            result = minimize(**kwargs)\n",
    "\n",
    "            if not singular:\n",
    "                results[i, :] = result.x\n",
    "                initial_guess = result.x\n",
    "            else:\n",
    "                results=result.x\n",
    "        return results\n",
    "\n",
    "    def _efficient_frontier_cvxpy(self, singular=None):\n",
    "        weights = cp.Variable(self.dim)\n",
    "        gamma_param = cp.Parameter(nonneg=False)\n",
    "        markowitz = 0.5 * cp.quad_form(weights, cp.psd_wrap(self.expected_covariance)) - gamma_param * self.expected_returns.T @ weights\n",
    "        constraints = [cp.sum(weights) == 1, weights >= 0]\n",
    "        problem = cp.Problem(cp.Minimize(markowitz), constraints)\n",
    "\n",
    "        if not singular:\n",
    "            results = np.zeros((len(self.gamma_linspace), self.dim))\n",
    "            itterator = enumerate(self.gamma_linspace)\n",
    "        else:\n",
    "            itterator = [(0, singular)]\n",
    "\n",
    "        for i, gamma in itterator:\n",
    "            gamma_param.value = gamma\n",
    "            problem.solve(warm_start=True)\n",
    "            if not singular:\n",
    "                results[i, :] = weights.value\n",
    "            else:\n",
    "                results = weights.value\n",
    "        return results\n",
    "\n",
    "    def get_expected_returns(self) -> pd.DataFrame | pd.Series:\n",
    "        #TODO: Attention! If extending beyond ERC, if statement must be updated.\n",
    "        if self.trust_markowitz and self.type == 'erc':\n",
    "            internal_expectations = np.array([portfolio.expected_portfolio_return for portfolio in Portfolio.non_combined_portfolios])\n",
    "            return pd.Series(internal_expectations, index=self.returns.columns)\n",
    "        return self.returns.mean(axis=0)\n",
    "    \n",
    "    def get_expected_covariance(self) -> pd.DataFrame | pd.Series:\n",
    "        if self.trust_markowitz and self.type == 'erc':\n",
    "            internal_expectations = np.array([np.sqrt(portfolio.expected_portfolio_varcov) for portfolio in Portfolio.non_combined_portfolios])\n",
    "            sample_correlations = self.returns.corr().fillna(0)\n",
    "            varcov_matrix = np.outer(internal_expectations, internal_expectations) * sample_correlations\n",
    "            return pd.DataFrame(varcov_matrix, index=self.returns.columns, columns=self.returns.columns)\n",
    "        return self.returns.cov(ddof=0)\n",
    "    \n",
    "    def get_expected_portfolio_return(self) -> float:\n",
    "        return np.dot(self.expected_returns, self.optimal_weights)\n",
    "    \n",
    "    def get_expected_portfolio_varcov(self) -> float:\n",
    "        return self.optimal_weights.T @ self.expected_covariance @ self.optimal_weights\n",
    "\n",
    "    def _resample(self, method) -> np.ndarray:\n",
    "        #TODO: Attention! Low number of simulations set for testing\n",
    "        N_SUMULATIONS = 2 # 500\n",
    "\n",
    "        original_moments = (self.expected_returns.copy(), self.expected_covariance.copy())\n",
    "        simulated_weights = []\n",
    "\n",
    "        for i in range(N_SUMULATIONS):\n",
    "            np.random.seed(i)\n",
    "            simulated_returns = np.random.multivariate_normal(self.expected_returns, self.expected_covariance, self.len)\n",
    "            # TODO: verify necessity of annualization factor\n",
    "            self.expected_returns = self._pandify(np.mean(simulated_returns, axis=0))# * ANNUALIZATION_FACTOR\n",
    "            self.expected_covariance = self._pandify(np.cov(simulated_returns.T, ddof=0))\n",
    "            self.optimal_weights = method()\n",
    "            simulated_weights.append(self.optimal_weights)\n",
    "        \n",
    "        self.expected_returns, self.expected_covariance = original_moments\n",
    "        combined_simulation_data = np.stack(simulated_weights, axis=0)\n",
    "        return combined_simulation_data.mean(axis=0) # mean across gammas\n",
    "    \n",
    "    def _pandify(self, array: np.ndarray) -> pd.Series | pd.DataFrame:\n",
    "        if array.ndim == 1:\n",
    "            return pd.Series(array, index=self.ticker)\n",
    "        else:\n",
    "            return pd.DataFrame(array, index=self.ticker, columns=self.ticker)\n",
    "    \n",
    "    def _fit_erc(self):\n",
    "        weights = cp.Variable(self.dim)\n",
    "        objective = cp.Minimize(0.5 * cp.quad_form(weights, self.expected_covariance))\n",
    "\n",
    "        log_constraint_bound = -self.dim * np.log(self.dim) - 2  # -2 does not matter after rescaling\n",
    "        log_constraint = cp.sum(cp.log(weights)) >= log_constraint_bound\n",
    "        constraints = [weights >= 0, weights <= 1, log_constraint]\n",
    "\n",
    "        problem = cp.Problem(objective, constraints)\n",
    "        # problem.solve(solver=cp.SCS, eps=1e-12) # Results in e-27 precision   \n",
    "        problem.solve(warm_start=True) # Results in e-27 precision    \n",
    "\n",
    "        if weights.value is None:\n",
    "            result = self._fit_erc_robust()\n",
    "        else:\n",
    "            result = weights.value / np.sum(weights.value)\n",
    "        return result\n",
    "\n",
    "    def _fit_erc_robust(self) -> np.ndarray:\n",
    "        print(\"ERC optimization failed to find a solution. Attempting robust optimization.\")\n",
    "        def _ERC(x, cov_matrix):\n",
    "            volatility = np.sqrt(x.T @ cov_matrix @ x)\n",
    "            abs_risk_contribution = x * (cov_matrix @ x) / volatility\n",
    "            mean = np.mean(abs_risk_contribution)\n",
    "            return np.sum((abs_risk_contribution - mean)**2)\n",
    "        \n",
    "        bounds = Bounds(0, 1)\n",
    "        lc = LinearConstraint(np.ones(self.dim), 1, 1)\n",
    "        settings = {'tol': 1e-16, 'method': 'SLSQP'} # This tolerance is required to match cvxpy results\n",
    "        res = minimize(_ERC, np.full(self.dim, 1/self.dim), args=(self.expected_covariance), constraints=[lc], bounds=bounds, **settings)\n",
    "        return res.x\n",
    "    \n",
    "    def evaluate_performance(self, evaluationData: pd.DataFrame | pd.Series) -> pd.Series:\n",
    "        # Returns Adjusted for Return-Shifted Weights\n",
    "        if evaluationData.isna().all().all() or (evaluationData == 0).all().all():\n",
    "            print(\"No data available for evaluation.\")\n",
    "            return pd.Series(0, index=evaluationData.index)\n",
    "        portfolioWeights = self.optimal_weights\n",
    "        subperiodReturns = []\n",
    "        subperiodWeights = [portfolioWeights]\n",
    "        for singleSubperiodReturns in evaluationData.values:\n",
    "            portfolioReturns = subperiodWeights[-1] @ singleSubperiodReturns\n",
    "            portfolioWeights = subperiodWeights[-1] * (1 + singleSubperiodReturns) / (1 + portfolioReturns)\n",
    "            subperiodReturns.append(portfolioReturns)\n",
    "            subperiodWeights.append(portfolioWeights)\n",
    "        self.actual_returns = pd.Series(subperiodReturns, index=evaluationData.index)\n",
    "        self.actual_weights = pd.DataFrame(subperiodWeights[:-1], index=evaluationData.index, columns=self.ticker)\n",
    "        return pd.Series(subperiodReturns, index=evaluationData.index)\n",
    "    \n",
    "    def log_visuals(self):\n",
    "        gammas = np.linspace(-0.5, 1.5, 101)\n",
    "        efficient_frontier = self.__class__.efficient_frontier(gammas, self.expected_returns, self.expected_covariance)\n",
    "        return efficient_frontier\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mspinner = Spinner(\"Starting...\")\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mspinner.start()\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mspinner.message(\"Loading data...\", \"blue\")\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m root \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18m__file__\u001b[39m)\n\u001b[1;32m      9\u001b[0m staticPath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStatic.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m ritPath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDS_RI_T_USD_M.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "spinner = Spinner(\"Starting...\")\n",
    "spinner.start()\n",
    "spinner.message(\"Loading data...\", \"blue\")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "root = os.path.dirname(__file__)\n",
    "staticPath = os.path.join(root, 'data', 'Static.xlsx')\n",
    "ritPath = os.path.join(root, 'data', 'DS_RI_T_USD_M.xlsx')\n",
    "mvPath = os.path.join(root, 'data', 'DS_MV_USD_M.xlsx')\n",
    "rfPath = os.path.join(root, 'data', 'Risk_Free_Rate.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "staticData = pd.read_excel(staticPath, engine='openpyxl')\n",
    "masterData = pd.read_excel(ritPath, usecols=lambda x: x != 'NAME', index_col=0, engine='openpyxl').transpose()\n",
    "masterData.index.rename('DATE', inplace=True) # print(sum(masterData.isna().any())) # Prices have no missing values\n",
    "masterData = masterData[masterData.index.year > 2000]\n",
    "\n",
    "capData = pd.read_excel(mvPath, usecols=lambda x: x != 'NAME', index_col=0, engine='openpyxl').transpose()\n",
    "capData.index = pd.to_datetime(capData.index, format='%Y-%m-%d')\n",
    "capData.index.rename('DATE', inplace=True)\n",
    "capData = capData[capData.index.year > 2000] * 1e6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "global masterIndex, global_tickers\n",
    "masterIndex = masterData.index\n",
    "global_tickers = list(masterData.columns)\n",
    "df_dict = {}\n",
    "mv_dict = {}\n",
    "for region in ['AMER', 'EM', 'EUR', 'PAC']:\n",
    "    filter = staticData['ISIN'][staticData['Region'] == region]\n",
    "    df_dict[region] = masterData[filter].pct_change()\n",
    "    mv_dict[region] = capData[filter]\n",
    "equity_returns = pd.concat(df_dict.values(), keys=df_dict.keys(), axis=1)\n",
    "market_values = pd.concat(mv_dict.values(), keys=mv_dict.keys(), axis=1)\n",
    "\n",
    "commodities = {\n",
    "    \"Gold\": \"GC=F\",\n",
    "    \"Silver\": \"SI=F\",}\n",
    "\n",
    "global_tickers.extend(commodities.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metal_returns = pd.DataFrame(index=masterIndex)\n",
    "for name, ticker in commodities.items():\n",
    "    data = yf.download(ticker, start=min(masterIndex), end=max(masterIndex), interval='1d')\n",
    "    data_filled = data['Close'].reindex(masterIndex).ffill()\n",
    "    metal_returns[name] = data_filled.pct_change()\n",
    "\n",
    "for df in [equity_returns, metal_returns]:\n",
    "    df.replace([np.inf, -np.inf, -1], np.nan, inplace=True)\n",
    "    df.fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "portfolio_keys = ['equity_amer', 'equity_em', 'equity_pac', 'equity_eur', 'metals']\n",
    "portfolio_returns = pd.DataFrame(index=masterIndex, columns=[*portfolio_keys, 'ERC'])\n",
    "portfolio_returns[:] = 0\n",
    "\n",
    "global_tickers.extend(portfolio_keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def iteration_depth(limit=None, frequency=\"annual\"):\n",
    "    if frequency == \"annual\":\n",
    "        if limit is None:\n",
    "            YYYY = 2021\n",
    "        else:\n",
    "            YYYY = limit\n",
    "        indexIterator = {0: {'optimizationIndex': masterIndex.year < 2006, 'evaluationIndex': masterIndex.year == 2006}}\n",
    "        for year, index in zip(range(2007, YYYY + 1), range(1, 22 + 1)):\n",
    "            optimizationIndex = (masterIndex.year < year) & (masterIndex.year >= 2000 + index)\n",
    "            evaluationIndex = masterIndex.year == year\n",
    "            indexIterator[index] = {'optimizationIndex': optimizationIndex, 'evaluationIndex': evaluationIndex}\n",
    "\n",
    "    elif frequency == \"monthly\":\n",
    "        if limit is None:\n",
    "            YYYY = 2021\n",
    "        else:\n",
    "            YYYY = limit\n",
    "        index = 0\n",
    "        indexIterator = {}\n",
    "        for year in range(2006, YYYY + 1):\n",
    "            for month in range(1, 13):\n",
    "                if year == YYYY and month > 1:\n",
    "                    break\n",
    "                # Calculate 5-year (60 months) rolling lookback period\n",
    "                start_year = year\n",
    "                start_month = month - 1\n",
    "                if start_month == 0:\n",
    "                    start_month = 12\n",
    "                    start_year -= 1\n",
    "                end_year = start_year - 5\n",
    "                end_month = start_month\n",
    "\n",
    "                optimizationIndex = (\n",
    "                    ((masterIndex.year > end_year) | \n",
    "                    ((masterIndex.year == end_year) & (masterIndex.month >= end_month))) &\n",
    "                    ((masterIndex.year < year) |\n",
    "                    ((masterIndex.year == year) & (masterIndex.month < month)))\n",
    "                )\n",
    "                evaluationIndex = (masterIndex.year == year) & (masterIndex.month == month)\n",
    "                indexIterator[index] = {'optimizationIndex': optimizationIndex, 'evaluationIndex': evaluationIndex}\n",
    "                index += 1\n",
    "    return indexIterator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'masterIndex' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m visual_data \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m----> 2\u001b[0m indexIterator \u001b[38;5;241m=\u001b[39m iteration_depth(frequency\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mannual\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m, in \u001b[0;36miteration_depth\u001b[0;34m(limit, frequency)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m     YYYY \u001b[38;5;241m=\u001b[39m limit\n\u001b[0;32m----> 7\u001b[0m indexIterator \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m0\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizationIndex\u001b[39m\u001b[38;5;124m'\u001b[39m: masterIndex\u001b[38;5;241m.\u001b[39myear \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2006\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevaluationIndex\u001b[39m\u001b[38;5;124m'\u001b[39m: masterIndex\u001b[38;5;241m.\u001b[39myear \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2006\u001b[39m}}\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m year, index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2007\u001b[39m, YYYY \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m22\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m      9\u001b[0m     optimizationIndex \u001b[38;5;241m=\u001b[39m (masterIndex\u001b[38;5;241m.\u001b[39myear \u001b[38;5;241m<\u001b[39m year) \u001b[38;5;241m&\u001b[39m (masterIndex\u001b[38;5;241m.\u001b[39myear \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2000\u001b[39m \u001b[38;5;241m+\u001b[39m index)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'masterIndex' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "visual_data = {}\n",
    "indexIterator = iteration_depth(frequency=\"annual\")\n",
    "#spinner.message('Optimizing', 'yellow')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "for step in indexIterator:\n",
    "    \n",
    "    #spinner.erase()\n",
    "    #spinner.message(f'Optimizing {step+1}/{len(indexIterator)}...', 'yellow')\n",
    "\n",
    "    optimizationIndex = indexIterator[step]['optimizationIndex']\n",
    "    evaluationIndex = indexIterator[step]['evaluationIndex']\n",
    "\n",
    "    sampleEquity = equity_returns.loc[optimizationIndex]\n",
    "    sampleMarketValues = market_values.loc[optimizationIndex]\n",
    "    sampleMetals = metal_returns.loc[optimizationIndex]\n",
    "    evaluationEquity = equity_returns.loc[evaluationIndex]\n",
    "    evaluationMetals = metal_returns.loc[evaluationIndex]\n",
    "\n",
    "    # nullFilter = create_filter_mask(sampleEquity)\n",
    "    minMarketCapThreshold = 0\n",
    "    maxMarketCapThreshold = np.inf\n",
    "    nullFilter = create_filter_mask(sampleEquity, sampleMarketValues, minMarketCapThreshold, maxMarketCapThreshold)\n",
    "    sampleEquity = sampleEquity.drop(columns=nullFilter)\n",
    "    evaluationEquity = evaluationEquity.drop(columns=nullFilter)\n",
    "    print(sampleEquity.shape)\n",
    "\n",
    "    # Equity and Commodities Portfolios\n",
    "    equityPortfolioAMER = Portfolio(sampleEquity['AMER'], 'max_sharpe')\n",
    "    equityPortfolioEM = Portfolio(sampleEquity['EM'], 'max_sharpe')\n",
    "    equityPortfolioEUR = Portfolio(sampleEquity['EUR'], 'max_sharpe')\n",
    "    equityPortfolioPAC = Portfolio(sampleEquity['PAC'], 'max_sharpe')\n",
    "    metalsPortfolio = Portfolio(sampleMetals, 'max_sharpe')\n",
    "\n",
    "    portfolio_returns.loc[evaluationIndex, 'equity_amer'] = equityPortfolioAMER.evaluate_performance(evaluationEquity['AMER']).values\n",
    "    portfolio_returns.loc[evaluationIndex, 'equity_em'] = equityPortfolioEM.evaluate_performance(evaluationEquity['EM']).values\n",
    "    portfolio_returns.loc[evaluationIndex, 'equity_eur'] = equityPortfolioEUR.evaluate_performance(evaluationEquity['EUR']).values\n",
    "    portfolio_returns.loc[evaluationIndex, 'equity_pac'] = equityPortfolioPAC.evaluate_performance(evaluationEquity['PAC']).values\n",
    "    portfolio_returns.loc[evaluationIndex, 'metals'] = metalsPortfolio.evaluate_performance(evaluationMetals).values\n",
    "\n",
    "    # ERC Portfolio\n",
    "    samplePortfolio = portfolio_returns.loc[optimizationIndex]\n",
    "    evaluationPortfolio = portfolio_returns.loc[evaluationIndex]\n",
    "\n",
    "    ercPortfolio = Portfolio(samplePortfolio[portfolio_keys], 'erc', trust_markowitz=False)\n",
    "\n",
    "    portfolio_returns.loc[evaluationIndex, 'ERC'] = ercPortfolio.evaluate_performance(evaluationPortfolio[portfolio_keys]).values\n",
    "\n",
    "    # Optional Visual Logging\n",
    "    portfolios = [equityPortfolioAMER, equityPortfolioEM, equityPortfolioEUR, equityPortfolioPAC, metalsPortfolio, ercPortfolio]\n",
    "    portfolio_names = portfolio_keys + ['ERC']\n",
    "    for portfolio, portfolio_name in zip(portfolios, portfolio_names):\n",
    "        tickers = portfolio.ticker\n",
    "        frontier = portfolio.frontier\n",
    "\n",
    "        expected_returns = frontier['expected_return'].values\n",
    "        expected_variances = frontier['expected_variance'].values\n",
    "        expected_sharpes = frontier['expected_sharpe'].values\n",
    "        weights = frontier.loc[:, tickers].values\n",
    "        \n",
    "        for i, gamma in enumerate(Portfolio.gamma_linspace):\n",
    "            row_data = [expected_returns[i], expected_variances[i], expected_sharpes[i]]\n",
    "            \n",
    "            weight_row = [np.nan] * len(global_tickers)\n",
    "            for j, asset in enumerate(tickers):\n",
    "                asset_index = global_tickers.index(asset)\n",
    "                weight_row[asset_index] = weights[i, j]\n",
    "            \n",
    "            row_data.extend(weight_row)\n",
    "            visual_data[(step, gamma, portfolio_name)] = row_data\n",
    "\n",
    "        Portfolio.non_combined_portfolios = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index = pd.MultiIndex.from_tuples(visual_data.keys(), names=[\"year\", \"gamma\", \"portfolio\"])\n",
    "columns = pd.MultiIndex.from_tuples(\n",
    "    [(\"metrics\", \"expected_return\"), (\"metrics\", \"expected_variance\"), (\"metrics\", \"expected_sharpe\")] +\n",
    "    [(\"weights\", asset) for asset in global_tickers],\n",
    "    names=[\"category\", \"attribute\"]\n",
    ")\n",
    "\n",
    "visual_df = pd.DataFrame.from_dict(visual_data, orient=\"index\", columns=columns)\n",
    "visual_df.index = index  # Set the MultiIndex\n",
    "years = visual_df.index.get_level_values(\"year\").unique()\n",
    "# We must break up the file into smaller chunks to use git\n",
    "for year in years:\n",
    "    yearly_data = visual_df.xs(year, level=\"year\")\n",
    "    yearly_data.to_hdf(f'data/efficient_frontiers_{year}.hdf', key='frontier_data', mode='w')\n",
    "\n",
    "\n",
    "# print(visual_df.loc[(slice(None), slice(None), 'equity_amer'), :].dropna(how='all', axis=1))\n",
    "# print(visual_df.loc[(slice(None), slice(None), 'ERC'), :].dropna(how='all', axis=1))\n",
    "    \n",
    "\n",
    "#spinner.erase()\n",
    "#spinner.message('Done!\\n', 'green')\n",
    "#spinner.stop()\n",
    "\n",
    "print((1 + portfolio_returns[portfolio_returns.index.year < 2022]).cumprod().tail(1))\n",
    "# sharpe ratio of portfolios\n",
    "print(portfolio_evaluation(portfolio_returns, pd.Series(0, index=portfolio_returns.index))['SR'])\n",
    "# print(portfolio_evaluation(portfolio_returns['metals'], pd.Series(0, index=portfolio_returns.index)))\n",
    "print(portfolio_evaluation(portfolio_returns['ERC'], pd.Series(0, index=portfolio_returns.index)))\n",
    "print(f\"Optimization Runtime: {(time.time() - start_time):2f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
